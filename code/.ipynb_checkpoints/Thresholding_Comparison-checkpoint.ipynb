{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from datautils import*\n",
    "from IPython.display import Markdown, display\n",
    "import numpy as np\n",
    "from preprocessing import*\n",
    "from learning import*\n",
    "from bootstrapping import*\n",
    "from bootstrapping_unsupervised import*\n",
    "from similarityutils import*\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# run thresholding comparison\n",
    "thresholding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start from here\n",
    "def thresholding():\n",
    "   \n",
    "    #set these parameters   \n",
    "    bootstrap_methods = ['attrelbow_density','attrstatic_density', 'attrotsu_density','attrvalley_density']\n",
    "    threshold_labels = ['elbow', 'static', 'Otsu\\'s', 'valley']\n",
    "    #threshold_colors = ['blue', 'green', 'orange', 'magenta']\n",
    "    threshold_colors = [ '#56B4E9',\"#E69F00\" ,'#009E73', '#CC79A7']\n",
    "    threshold_linestyles = ['dotted', None, 'dashdot', '--']\n",
    "    threshold_text = [50, 100, 150, 200]\n",
    "    thresholds = []\n",
    "    model = 'rf'\n",
    "    dataPathmain = \"../datasets/author\"\n",
    "    domain = 'product'\n",
    "    datasets = [ 'DBPediaAuthors_DnbDataAuthors']\n",
    "        \n",
    "      \n",
    "    for dataset in datasets:\n",
    "        model_ = []\n",
    "        method_ = []\n",
    "        precision_=[]\n",
    "        precision_sigma=[]\n",
    "        recall_=[]\n",
    "        recall_sigma=[]\n",
    "        f1_=[]\n",
    "        f1_sigma = []\n",
    "        bootstrap_sample_correctness = []\n",
    "        bootstrap_sample_f1 = []\n",
    "\n",
    "        dataset_=[]\n",
    "        \n",
    "        #get data and profiling information\n",
    "        display(\"Unsupervised Matching: %s\" %dataset)\n",
    "        \n",
    "        featureFile_train = dataPathmain+'/features_'+dataset+'_train'\n",
    "        featureFile_test = dataPathmain+'/features_'+dataset+'_test'\n",
    "        \n",
    "        print(\"Stats about pool data\")\n",
    "        trainingData = getLabelledDataFromFile(featureFile_train, rescale=True)\n",
    "        print(\"Profiling Density\")\n",
    "        prof_results = dict()\n",
    "        data_values = trainingData['feature_values']\n",
    "        tobedropped=[]\n",
    "        for c in data_values.columns:\n",
    "            empty_values = len(data_values[data_values[c] == -1])\n",
    "            per= float(empty_values)/float(len(data_values[c]))\n",
    "            density = 1-per\n",
    "           \n",
    "            #drop non-dense attributes for the wdc datasets\n",
    "            if 'wdc_product' in domain:\n",
    "                column_name = c\n",
    "            else: column_name = c.split(\"_\")[0]\n",
    "            if (density<0.1):tobedropped.append(c)\n",
    "            else: prof_results[column_name]= (\"%.3f\" %density)\n",
    "        \n",
    "        if 'wdc_product' in domain:\n",
    "            print (\"Columns to be dropped:\", len(tobedropped))\n",
    "            trainingData['feature_values'] = trainingData['feature_values'].drop(tobedropped, axis=1)\n",
    "        display(prof_results)\n",
    "        print(\"Stats about validation data\")\n",
    "        validationData = getLabelledDataFromFile(featureFile_test, rescale=True)\n",
    "        if 'wdc_product' in domain:\n",
    "            validationData['feature_values'] = validationData['feature_values'].drop(tobedropped, axis=1)\n",
    "\n",
    "        \n",
    "        X = trainingData['feature_values']\n",
    "        y = trainingData['labels']\n",
    "        ids  = trainingData['ids']\n",
    "        bootstrap_sample_correctness\n",
    "    \n",
    "        #get results for supervised matching\n",
    "        prec, recall, fscore, support = batchTraining(X,y,validationData['feature_values'],validationData['labels'],model, printResults = False,\n",
    "                                                           optimization=False)\n",
    "        precision_.append(\"%.3f\" %prec)\n",
    "        recall_.append(\"%.3f\" %recall)\n",
    "        f1_.append(\"%.3f\" %fscore)\n",
    "        model_.append(model)\n",
    "        \n",
    "        dataset_.append(dataset)\n",
    "        method_.append('Passive Supervised Learning')\n",
    "        bootstrap_sample_correctness.append(\"-\")\n",
    "        bootstrap_sample_f1.append(\"-\")\n",
    "        for m in bootstrap_methods:\n",
    "\n",
    "            bootstrap = BootstrappingUnsupervised(data=X, labels=y, ids=ids, bootstrap_method=m, domain=domain)\n",
    "            sorted_dataset = bootstrap.sorted_dataset\n",
    "            thresholds.append(bootstrap.threshold)\n",
    "\n",
    "            bootstrapping_sample = bootstrap.sample\n",
    "            bootstrap_sample_correctness.append(bootstrapping_sample['correctness'])\n",
    "            bootstrap_sample_f1.append(bootstrapping_sample['f1'])\n",
    "\n",
    "            prec, recall, fscore, support = batchTraining(bootstrapping_sample['data'],bootstrapping_sample['labels'],\n",
    "                                                          validationData['feature_values'],validationData['labels'],model, printResults = False, optimization=False,\n",
    "                                                          showMisclassifications = False, ids = ids)\n",
    "\n",
    "            print(\"Method %s gives %f prec, %f recall and %f f1 if noisy data are used to train a RF\" % (m, prec, recall, fscore))\n",
    "            \n",
    "            method_.append(m)\n",
    "            model_.append(model)\n",
    "            precision_.append(\"%.3f\" %prec)\n",
    "\n",
    "            recall_.append(\"%.3f\" %recall)\n",
    "\n",
    "            f1_.append(\"%.3f\" %fscore)\n",
    "\n",
    "            dataset_.append(dataset)\n",
    "\n",
    "        results = list(zip(dataset_ ,bootstrap_sample_correctness, bootstrap_sample_f1,method_,precision_,recall_, f1_))\n",
    "        df_results = pd.DataFrame(results, columns = ['Dataset',  'Bootstrap sample correctness', 'Bootstrap sample f1','Method', 'Precision', 'Recall','F1- Random Forest']) \n",
    "        display(df_results)\n",
    "        \n",
    "        simple_hist, _ = np.histogram(sorted_dataset, bins=100, range=(0.0, 1.0))\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(np.arange(0.00,1.0,0.01),simple_hist, c='#000000')\n",
    "        for i in range(len(threshold_colors)):\n",
    "            ax.axvline(x=thresholds[i], c= threshold_colors[i], linestyle =  threshold_linestyles[i], label=threshold_labels[i])\n",
    "            #ax.text(thresholds[i],threshold_text[i],threshold_labels[i],rotation=90)\n",
    "        \n",
    "        ax.legend(fontsize=12)\n",
    "        plt.xlabel('similarity scores')\n",
    "        plt.ylabel('# record pairs')\n",
    "        plt.savefig('../results/graphs/%s_threshold_comparison.pdf' % dataset, bbox_inches='tight', format='pdf')\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
